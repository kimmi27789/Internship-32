{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a66d3409",
   "metadata": {},
   "source": [
    "# 1) Program to display all the header tags from wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f18c398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in e:\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in e:\\anaconda3\\lib\\site-packages (from bs4) (4.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in e:\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.2.1)\n",
      "Requirement already satisfied: requests in e:\\anaconda3\\lib\\site-packages (2.26.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in e:\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in e:\\anaconda3\\lib\\site-packages (from requests) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\anaconda3\\lib\\site-packages (from requests) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\anaconda3\\lib\\site-packages (from requests) (2021.10.8)\n"
     ]
    }
   ],
   "source": [
    "#installing libraries\n",
    "!pip install bs4 \n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70cd0084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H1 tags: ['Wikipedia  The Free Encyclopedia']\n",
      "H2 tags: ['1 000 000+   articles', '100 000+   articles', '10 000+   articles', '1 000+   articles', '100+   articles']\n"
     ]
    }
   ],
   "source": [
    "#importing the required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "#send get request to webpage server\n",
    "page = requests.get(\"https://www.wikipedia.org/\")\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "#initialize the header tag list as empty\n",
    "h1_tags = []\n",
    "h2_tags = []\n",
    "\n",
    "#loop in all h1 tags found on page\n",
    "for i in soup.find_all('h1', class_ = \"central-textlogo-wrapper\"):\n",
    "    h1_tags.append(i.get_text().strip().replace('\\n',\" \"))\n",
    "print (\"H1 tags:\",h1_tags)\n",
    "\n",
    "#loop in all h2 tags found on page  \n",
    "for i in soup.find_all('h2', class_ = \"bookshelf-container\"):\n",
    "    h2_tags.append(i.get_text().strip().replace(\"\\n\",\" \").replace(u'\\xa0', ' '))\n",
    "print (\"H2 tags:\",h2_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd83c69d",
   "metadata": {},
   "source": [
    "# 2) To display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3170b162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie Name</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Releasing Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>9.3</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>9.2</td>\n",
       "      <td>1972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Lord of the Rings: The Return of the King</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Schindler's List</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>North by Northwest</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Vertigo</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Singin' in the Rain</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Citizen Kane</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>M - Eine Stadt sucht einen Mörder</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Movie Name Rating Releasing Date\n",
       "0                        The Shawshank Redemption    9.3           1994\n",
       "1                                   The Godfather    9.2           1972\n",
       "2                                 The Dark Knight    9.0           2008\n",
       "3   The Lord of the Rings: The Return of the King    9.0           2003\n",
       "4                                Schindler's List    9.0           1993\n",
       "..                                            ...    ...            ...\n",
       "95                             North by Northwest    8.3           1959\n",
       "96                                        Vertigo    8.3           1958\n",
       "97                            Singin' in the Rain    8.3           1952\n",
       "98                                   Citizen Kane    8.3           1941\n",
       "99              M - Eine Stadt sucht einen Mörder    8.3           1931\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As the data is distributed under two pages (50 rows on each page) for IMDB's top rated 100 movies\n",
    "# set counter as num where num represents a serial number and \n",
    "# if it goes beyond 50 we will switch the request URL to scrape the rows 51-100 \n",
    "\n",
    "#import pandas to create a dataframe using Observed results\n",
    "import pandas as pd\n",
    "\n",
    "#initialize the movie name, rating and year_of_release list as empty\n",
    "movie_name = []\n",
    "movie_rating = []\n",
    "movie_releasing_year = []\n",
    "\n",
    "num = 1\n",
    "while(num <= 100):\n",
    "    #send get request to webpage server\n",
    "    if(num <= 50):\n",
    "        #load page1 carrrying rows 1-50\n",
    "        page = requests.get(\"https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc\")\n",
    "        soup = BeautifulSoup(page.content)\n",
    "    else:\n",
    "        #load page2 carrrying rows 51-100\n",
    "        page = requests.get(\"https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc&start=51&ref_=adv_nxt\")\n",
    "        soup = BeautifulSoup(page.content)\n",
    "        \n",
    "    #loop in all movie names and releasing year for top 50 entries found on page\n",
    "    for i in soup.find_all('h3', class_ = \"lister-item-header\"):\n",
    "        movie_name.append(i.find('a').get_text().strip())\n",
    "        movie_releasing_year.append(i.find('span', class_=\"lister-item-year text-muted unbold\").get_text().strip().replace(\"(\",\"\").replace(\")\",\"\"))\n",
    "        num = num + 1\n",
    "\n",
    "    #loop in all movie names for top 100 entries found on page\n",
    "    for i in soup.find_all('div', class_ = \"inline-block ratings-imdb-rating\"):\n",
    "        movie_rating.append(i.find(\"strong\").get_text().strip())\n",
    "    \n",
    "#create a datagframe using above lists\n",
    "df = pd.DataFrame({'Movie Name':movie_name, 'Rating':movie_rating, 'Releasing Date':movie_releasing_year})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed24737",
   "metadata": {},
   "source": [
    "# 3) To display IMDB’s Top rated 100 Indian movies’ data (i.e. name, rating, year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25ef9bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie Name</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Releasing Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rocketry: The Nambi Effect</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anbe Sivam</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jai Bhim</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nayakan</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Golmaal</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Ustad Hotel</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>The Legend of Bhagat Singh</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Virumandi</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Baahubali 2: The Conclusion</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Angoor</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Movie Name Rating Releasing Date\n",
       "0    Rocketry: The Nambi Effect    8.5           2022\n",
       "1                    Anbe Sivam    8.4           2003\n",
       "2                      Jai Bhim    8.4           2021\n",
       "3                       Nayakan    8.4           1987\n",
       "4                       Golmaal    8.4           1979\n",
       "..                          ...    ...            ...\n",
       "95                  Ustad Hotel    8.0           2012\n",
       "96   The Legend of Bhagat Singh    8.0           2002\n",
       "97                    Virumandi    8.0           2004\n",
       "98  Baahubali 2: The Conclusion    8.0           2017\n",
       "99                       Angoor    8.0           1982\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I didn't found any specific link for IMDB’s Top rated 100 Indian movies like Problem Statement - 2 \n",
    "# Instead of that I found IMDB’s Top rated 250 Indian movies, so m showing top 100 results for that page\n",
    "\n",
    "#import pandas to create a dataframe using Observed results\n",
    "import pandas as pd\n",
    "\n",
    "#send get request to webpage server\n",
    "page = requests.get(\"https://www.imdb.com/india/top-rated-indian-movies/\")\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "#initialize the movie name, rating and year_of_release list as empty\n",
    "movie_name = []\n",
    "movie_rating = []\n",
    "movie_releasing_year = []\n",
    "        \n",
    "#loop in all movie names and releasing year for top 100 entries found on page\n",
    "for i in soup.find_all('td', class_ = \"titleColumn\")[0:100]:\n",
    "    movie_name.append(i.find('a').get_text().strip())\n",
    "    movie_releasing_year.append(i.find('span', class_=\"secondaryInfo\").get_text().strip().replace(\"(\",\"\").replace(\")\",\"\"))\n",
    "    \n",
    "#loop in all movie names for top 100 entries found on page\n",
    "for i in soup.find_all('td', class_ = \"ratingColumn imdbRating\")[0:100]:\n",
    "    movie_rating.append(i.find(\"strong\").get_text().strip())\n",
    "\n",
    "#create a datagframe using above lists\n",
    "df = pd.DataFrame({'Movie Name':movie_name, 'Rating':movie_rating, 'Releasing Date':movie_releasing_year})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56600fd",
   "metadata": {},
   "source": [
    "# 4) To display list of respected former presidents of India(i.e. Name , Term of office) from https://presidentofindia.nic.in/former-presidents.htm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8139cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>President Name</th>\n",
       "      <th>Term of Office</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shri Ram Nath Kovind</td>\n",
       "      <td>25 July, 2017 to 25 July, 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shri Pranab Mukherjee</td>\n",
       "      <td>25 July, 2012 to 25 July, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Smt Pratibha Devisingh Patil</td>\n",
       "      <td>25 July, 2007 to 25 July, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DR. A.P.J. Abdul Kalam</td>\n",
       "      <td>25 July, 2002 to 25 July, 2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shri K. R. Narayanan</td>\n",
       "      <td>25 July, 1997 to 25 July, 2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dr Shankar Dayal Sharma</td>\n",
       "      <td>25 July, 1992 to 25 July, 1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shri R Venkataraman</td>\n",
       "      <td>25 July, 1987 to 25 July, 1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Giani Zail Singh</td>\n",
       "      <td>25 July, 1982 to 25 July, 1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Shri Neelam Sanjiva Reddy</td>\n",
       "      <td>25 July, 1977 to 25 July, 1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dr. Fakhruddin Ali Ahmed</td>\n",
       "      <td>24 August, 1974 to 11 February, 1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Shri Varahagiri Venkata Giri</td>\n",
       "      <td>3 May, 1969 to 20 July, 1969 and 24 August, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Dr. Zakir Husain</td>\n",
       "      <td>13 May, 1967 to 3 May, 1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Dr. Sarvepalli Radhakrishnan</td>\n",
       "      <td>13 May, 1962 to 13 May, 1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Dr. Rajendra Prasad</td>\n",
       "      <td>26 January, 1950 to 13 May, 1962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   President Name  \\\n",
       "0           Shri Ram Nath Kovind    \n",
       "1          Shri Pranab Mukherjee    \n",
       "2   Smt Pratibha Devisingh Patil    \n",
       "3         DR. A.P.J. Abdul Kalam    \n",
       "4           Shri K. R. Narayanan    \n",
       "5        Dr Shankar Dayal Sharma    \n",
       "6            Shri R Venkataraman    \n",
       "7               Giani Zail Singh    \n",
       "8      Shri Neelam Sanjiva Reddy    \n",
       "9       Dr. Fakhruddin Ali Ahmed    \n",
       "10  Shri Varahagiri Venkata Giri    \n",
       "11              Dr. Zakir Husain    \n",
       "12  Dr. Sarvepalli Radhakrishnan    \n",
       "13           Dr. Rajendra Prasad    \n",
       "\n",
       "                                       Term of Office  \n",
       "0                      25 July, 2017 to 25 July, 2022  \n",
       "1                      25 July, 2012 to 25 July, 2017  \n",
       "2                      25 July, 2007 to 25 July, 2012  \n",
       "3                      25 July, 2002 to 25 July, 2007  \n",
       "4                      25 July, 1997 to 25 July, 2002  \n",
       "5                      25 July, 1992 to 25 July, 1997  \n",
       "6                      25 July, 1987 to 25 July, 1992  \n",
       "7                      25 July, 1982 to 25 July, 1987  \n",
       "8                      25 July, 1977 to 25 July, 1982  \n",
       "9                24 August, 1974 to 11 February, 1977  \n",
       "10   3 May, 1969 to 20 July, 1969 and 24 August, 1...  \n",
       "11                        13 May, 1967 to 3 May, 1969  \n",
       "12                       13 May, 1962 to 13 May, 1967  \n",
       "13                   26 January, 1950 to 13 May, 1962  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import pandas to create a dataframe using Observed results\n",
    "import pandas as pd\n",
    "\n",
    "#send get request to webpage server\n",
    "page = requests.get(\"https://presidentofindia.nic.in/former-presidents.htm\")\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "#initialize the President name and Term of Office list as empty\n",
    "president_name = []\n",
    "term_of_office = []\n",
    "\n",
    "#find all president names found on page\n",
    "for i in soup.find_all('div', class_ = \"presidentListing\"):\n",
    "    president_name.append(i.find('h3').get_text().strip().split(\"(\")[0])\n",
    "    term_of_office.append(i.find('p').get_text().strip().split(\":\")[1])\n",
    "    \n",
    "#create a datagframe using above lists\n",
    "df = pd.DataFrame({'President Name':president_name, 'Term of Office':term_of_office})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e747a9e5",
   "metadata": {},
   "source": [
    "# 5 To scrape cricket rankings from icc-cricket.com.\n",
    "            a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "            b) Top 10 ODI Batsmen along with the records of their team and rating.\n",
    "            c) Top 10 ODI bowlers along with the records of their team and rating.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f37f6755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOP 10 ODI Teams MALE\n",
      "\n",
      "           Team Matches Points Rating\n",
      "0       England      27  3,226    119\n",
      "1   New Zealand      22  2,508    114\n",
      "2         India      31  3,447    111\n",
      "3      Pakistan      22  2,354    107\n",
      "4     Australia      29  3,071    106\n",
      "5  South Africa      21  2,111    101\n",
      "6    Bangladesh      30  2,753     92\n",
      "7     Sri Lanka      29  2,658     92\n",
      "8   West Indies      41  2,902     71\n",
      "9   Afghanistan      18  1,238     69\n",
      "\n",
      "TOP 10 ODI Male Batsmen\n",
      "\n",
      "                  Player Team Rating\n",
      "0             Babar Azam  PAK    890\n",
      "1  Rassie van der Dussen   SA    789\n",
      "2        Quinton de Kock   SA    784\n",
      "3            Imam-ul-Haq  PAK    779\n",
      "4            Virat Kohli  IND    744\n",
      "5           Rohit Sharma  IND    740\n",
      "6         Jonny Bairstow  ENG    732\n",
      "7           David Warner  AUS    725\n",
      "8            Ross Taylor   NZ    701\n",
      "9            Steve Smith  AUS    697\n",
      "\n",
      "TOP 10 ODI Male Bowlers\n",
      "\n",
      "             Player Team Rating\n",
      "0       Trent Boult   NZ    775\n",
      "1    Josh Hazlewood  AUS    718\n",
      "2  Mujeeb Ur Rahman  AFG    676\n",
      "3    Jasprit Bumrah  IND    662\n",
      "4    Shaheen Afridi  PAK    661\n",
      "5     Mohammad Nabi  AFG    657\n",
      "6      Mehedi Hasan  BAN    655\n",
      "7        Matt Henry   NZ    654\n",
      "8    Mitchell Starc  AUS    653\n",
      "9       Rashid Khan  AFG    651\n"
     ]
    }
   ],
   "source": [
    "#import pandas to create a dataframe using Observed results\n",
    "import pandas as pd\n",
    "\n",
    "#implementing part (a)----------------top 10 ODI teams-----------------------------------------------------------------------------------\n",
    "\n",
    "#send get request to webpage server\n",
    "page = requests.get(\"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\")\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "#initialize the matches, points and ratings list as empty\n",
    "team = []\n",
    "matches = []\n",
    "points = []\n",
    "ratings = []\n",
    "\n",
    "#appending top team\n",
    "team.append(soup.find('td',class_='rankings-block__banner--team-name').get_text().strip().split('\\n')[0])\n",
    "matches.append(soup.find('td',class_='rankings-block__banner--matches').get_text().strip())\n",
    "points.append(soup.find('td', class_='rankings-block__banner--points').get_text().strip())\n",
    "ratings.append(soup.find('td',class_='rankings-block__banner--rating u-text-right').get_text().strip())\n",
    "\n",
    "#appending other top 9 teams on page\n",
    "for row in soup.find_all('tr', class_ = \"table-body\")[0:9]:\n",
    "    col = row.find_all('td')\n",
    "    team.append(col[1].get_text().strip().split('\\n')[0])\n",
    "    matches.append(col[2].get_text().strip())\n",
    "    points.append(col[3].get_text().strip())\n",
    "    ratings.append(col[4].get_text().strip())\n",
    "\n",
    "    \n",
    "#create a dataframe using above lists\n",
    "df_top_10_odi_team_male = pd.DataFrame({'Team':team, 'Matches':matches, 'Points':points, 'Rating':ratings})\n",
    "print(\"\\nTOP 10 ODI Teams MALE\\n\")\n",
    "print(df_top_10_odi_team_male)\n",
    "\n",
    "\n",
    "\n",
    "#implementing part (b) and (c)----------top 10 ODI batsman and bowlers---------------------------------------------------------------\n",
    "\n",
    "#send get request to webpage server\n",
    "page = requests.get(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi\")\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "#initialize the matches, points and ratings list as empty\n",
    "player_name = []\n",
    "team_name = []\n",
    "rating = []\n",
    "\n",
    "count = 1\n",
    "#find all similar divisions on page\n",
    "for i in soup.find_all('div',class_='col-4 col-12-desk touch-scroll-list__element')[0:2]:\n",
    "    \n",
    "    #clearing all lists\n",
    "    player_name.clear()\n",
    "    team_name.clear()\n",
    "    rating.clear()\n",
    "    \n",
    "    #appending the top player details\n",
    "\n",
    "    player_name.append(i.find('div', class_='rankings-block__banner--name').get_text().strip())\n",
    "    team_name.append(i.find('div', class_='rankings-block__banner--nationality').get_text().strip().split('\\n')[0])\n",
    "    rating.append(i.find('div',class_='rankings-block__banner--rating').get_text().strip())\n",
    "    \n",
    "    #appending other top 9 players \n",
    "    for row in i.find_all('tr', class_ = \"table-body\")[0:9]:\n",
    "        col = row.find_all('td')\n",
    "        player_name.append(col[1].get_text().strip().split('\\n')[0])\n",
    "        team_name.append(col[2].find('span',class_='table-body__logo-text').get_text().strip())\n",
    "        rating.append(col[3].get_text().strip())\n",
    "\n",
    "\n",
    "    #create a dataframe using above lists\n",
    "    df_top_10_odi_male_player = pd.DataFrame({'Player':player_name, 'Team':team_name, 'Rating':rating})\n",
    "    \n",
    "    if(count == 1):\n",
    "        print(\"\\nTOP 10 ODI Male Batsmen\\n\")\n",
    "        print(df_top_10_odi_male_player)\n",
    "    else:\n",
    "        print(\"\\nTOP 10 ODI Male Bowlers\\n\")\n",
    "        print(df_top_10_odi_male_player)\n",
    "        \n",
    "    count = count + 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586263aa",
   "metadata": {},
   "source": [
    "# 6) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "    a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "    b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "    c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e18933d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOP 10 ODI Teams Women\n",
      "\n",
      "           Team Matches Points Rating\n",
      "0     Australia      29  4,837    167\n",
      "1       England      34  4,097    121\n",
      "2  South Africa      35  4,157    119\n",
      "3         India      33  3,392    103\n",
      "4   New Zealand      32  3,161     99\n",
      "5   West Indies      31  2,815     91\n",
      "6    Bangladesh      12    930     78\n",
      "7      Pakistan      30  1,962     65\n",
      "8       Ireland      11    516     47\n",
      "9     Sri Lanka      11    495     45\n",
      "\n",
      "TOP 10 ODI Women Batsmen\n",
      "\n",
      "                Player Team Rating\n",
      "0         Alyssa Healy  AUS    785\n",
      "1          Beth Mooney  AUS    749\n",
      "2       Natalie Sciver  ENG    740\n",
      "3      Laura Wolvaardt   SA    732\n",
      "4          Meg Lanning  AUS    710\n",
      "5       Rachael Haynes  AUS    701\n",
      "6      Smriti Mandhana  IND    698\n",
      "7    Amy Satterthwaite   NZ    681\n",
      "8     Harmanpreet Kaur  IND    662\n",
      "9  Chamari Athapaththu   SL    655\n",
      "\n",
      "TOP 10 ODI Women All-Rounders\n",
      "\n",
      "              Player Team Rating\n",
      "0       Ellyse Perry  AUS    374\n",
      "1     Natalie Sciver  ENG    372\n",
      "2     Marizanne Kapp   SA    349\n",
      "3    Hayley Matthews   WI    339\n",
      "4        Amelia Kerr   NZ    336\n",
      "5      Deepti Sharma  IND    271\n",
      "6   Ashleigh Gardner  AUS    270\n",
      "7      Jess Jonassen  AUS    246\n",
      "8     Jhulan Goswami  IND    219\n",
      "9  Sophie Ecclestone  ENG    217\n"
     ]
    }
   ],
   "source": [
    "#import pandas to create a dataframe using Observed results\n",
    "import pandas as pd\n",
    "\n",
    "#implementing part (a)----------------top 10 ODI teams women-----------------------------------------------------------------------------------\n",
    "\n",
    "#send get request to webpage server\n",
    "page = requests.get(\"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\")\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "#initialize the matches, points and ratings list as empty\n",
    "team = []\n",
    "matches = []\n",
    "points = []\n",
    "ratings = []\n",
    "\n",
    "#appending top team\n",
    "team.append(soup.find('td',class_='rankings-block__banner--team-name').get_text().strip().split('\\n')[0])\n",
    "matches.append(soup.find('td',class_='rankings-block__banner--matches').get_text().strip())\n",
    "points.append(soup.find('td', class_='rankings-block__banner--points').get_text().strip())\n",
    "ratings.append(soup.find('td',class_='rankings-block__banner--rating u-text-right').get_text().strip())\n",
    "\n",
    "#appending other top 9 teams on page\n",
    "for row in soup.find_all('tr', class_ = \"table-body\")[0:9]:\n",
    "    col = row.find_all('td')\n",
    "    team.append(col[1].get_text().strip().split('\\n')[0])\n",
    "    matches.append(col[2].get_text().strip())\n",
    "    points.append(col[3].get_text().strip())\n",
    "    ratings.append(col[4].get_text().strip())\n",
    "\n",
    "    \n",
    "#create a dataframe using above lists\n",
    "df_top_10_odi_team_female = pd.DataFrame({'Team':team, 'Matches':matches, 'Points':points, 'Rating':ratings})\n",
    "print(\"\\nTOP 10 ODI Teams Women\\n\")\n",
    "print(df_top_10_odi_team_female)\n",
    "\n",
    "\n",
    "\n",
    "#implementing part (b) and (c)----------top 10 ODI batsman and all-rounders---------------------------------------------------------------\n",
    "\n",
    "#send get request to webpage server\n",
    "page = requests.get(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi\")\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "#initialize the matches, points and ratings list as empty\n",
    "player_name = []\n",
    "team_name = []\n",
    "rating = []\n",
    "\n",
    "#find all similar divisions on page\n",
    "count = 1\n",
    "for i in soup.find_all('div',class_='col-4 col-12-desk uniform-grid__section touch-scroll-list__element')[0:3]:\n",
    "    if(count == 2):\n",
    "        count = count + 1\n",
    "        continue\n",
    "    #flushing all lists\n",
    "    player_name.clear()\n",
    "    team_name.clear()\n",
    "    rating.clear()\n",
    "    \n",
    "    #appending the top player details\n",
    "\n",
    "    player_name.append(i.find('div', class_='rankings-block__banner--name').get_text().strip())\n",
    "    team_name.append(i.find('div', class_='rankings-block__banner--nationality').get_text().strip().split('\\n')[0])\n",
    "    rating.append(i.find('div',class_='rankings-block__banner--rating').get_text().strip())\n",
    "    \n",
    "    #appending other top 9 players \n",
    "    for row in i.find_all('tr', class_ = \"table-body\")[0:9]:\n",
    "        col = row.find_all('td')\n",
    "        player_name.append(col[1].get_text().strip().split('\\n')[0])\n",
    "        team_name.append(col[2].find('span',class_='table-body__logo-text').get_text().strip())\n",
    "        rating.append(col[3].get_text().strip())\n",
    "\n",
    "\n",
    "    #create a dataframe using above lists\n",
    "    df_top_10_odi_female_player = pd.DataFrame({'Player':player_name, 'Team':team_name, 'Rating':rating})\n",
    "    \n",
    "    if(count == 1):\n",
    "        print(\"\\nTOP 10 ODI Women Batsmen\\n\")\n",
    "        print(df_top_10_odi_female_player)\n",
    "    else:\n",
    "        print(\"\\nTOP 10 ODI Women All-Rounders\\n\")\n",
    "        print(df_top_10_odi_female_player)\n",
    "        \n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a80d8d",
   "metadata": {},
   "source": [
    "# 7) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world :Headline, TimeNews, Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "905b3312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Time</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Biden administration awards $1.5 billion to fi...</td>\n",
       "      <td>57 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/24/biden-administ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Top 10 cities with the best pizzerias worldwid...</td>\n",
       "      <td>1 Hour Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/24/top-10-cities-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Black Girls in Trader Joe’s creator shares her...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/24/easy-meals-for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To raise more resilient kids, start by creatin...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/24/how-to-raise-r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why the airline climate change plan is trailin...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/24/how-airlines-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>'Queer Eye's Karamo Brown on the morning routi...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/24/queer-eyes-kar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>These 7 states have the least air pollution in...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/24/vermont-new-me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>New York is now No. 1 port in a tipping point ...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/24/new-york-now-n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Analysts have 'high conviction' that these sto...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/24/analysts-name-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Feared stock market bottom retest is now underway</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/24/feared-stock-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The No. 1 best city to retire isn't in Florida...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/24/wallethub-top-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The perils and promise of quantum computing ar...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/24/quantum-invest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Everything parents need to know about student ...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/24/what-parent-pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Check in, smoke up and tune out: Cannabis-frie...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/24/check-in-smoke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Convertibles drive into the sunset as automake...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/24/convertible-sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Britain's sudden lurch to 'Reaganomics' gets a...</td>\n",
       "      <td>9 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/24/liz-truss-brit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Here's our plan for Monday after another painf...</td>\n",
       "      <td>20 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/23/it-was-another...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What to watch in the markets in the week ahead</td>\n",
       "      <td>20 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/23/stocks-could-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Pro Picks: Watch all of Friday's big stock cal...</td>\n",
       "      <td>20 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/23/pro-picks-watc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13 careers where over 50% of workers are happy...</td>\n",
       "      <td>20 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/23/careers-where-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>New York AG wrongly said Yankees game on Apple...</td>\n",
       "      <td>20 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/23/new-york-ag-wr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Tech stocks just had the worst two-week stretc...</td>\n",
       "      <td>21 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/23/tech-stocks-wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Takeaways from Jim Cramer's interviews with th...</td>\n",
       "      <td>22 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/23/jim-cramer-sat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Some homebuyers are facing mortgage 'payment s...</td>\n",
       "      <td>22 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/23/some-homebuyer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Ether is down almost 20% since the merge. Here...</td>\n",
       "      <td>22 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/23/ether-is-down-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Elon Musk has over 20 direct reports at Tesla ...</td>\n",
       "      <td>22 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/23/elon-musk-dire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Trump SPAC shares are now around $16 after hit...</td>\n",
       "      <td>23 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/23/trump-merger-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Here's why U.S. fiscal policy is undermining t...</td>\n",
       "      <td>23 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/23/us-fiscal-poli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Why gold and crypto haven't proven to be 'infl...</td>\n",
       "      <td>23 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/23/why-gold-and-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Amid U.S. recession fears, these 4 steps can h...</td>\n",
       "      <td>23 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/23/amid-us-recess...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Headline          Time  \\\n",
       "0   Biden administration awards $1.5 billion to fi...    57 Min Ago   \n",
       "1   Top 10 cities with the best pizzerias worldwid...    1 Hour Ago   \n",
       "2   Black Girls in Trader Joe’s creator shares her...   2 Hours Ago   \n",
       "3   To raise more resilient kids, start by creatin...   3 Hours Ago   \n",
       "4   Why the airline climate change plan is trailin...   3 Hours Ago   \n",
       "5   'Queer Eye's Karamo Brown on the morning routi...   3 Hours Ago   \n",
       "6   These 7 states have the least air pollution in...   4 Hours Ago   \n",
       "7   New York is now No. 1 port in a tipping point ...   4 Hours Ago   \n",
       "8   Analysts have 'high conviction' that these sto...   4 Hours Ago   \n",
       "9   Feared stock market bottom retest is now underway   4 Hours Ago   \n",
       "10  The No. 1 best city to retire isn't in Florida...   4 Hours Ago   \n",
       "11  The perils and promise of quantum computing ar...   5 Hours Ago   \n",
       "12  Everything parents need to know about student ...   5 Hours Ago   \n",
       "13  Check in, smoke up and tune out: Cannabis-frie...   5 Hours Ago   \n",
       "14  Convertibles drive into the sunset as automake...   5 Hours Ago   \n",
       "15  Britain's sudden lurch to 'Reaganomics' gets a...   9 Hours Ago   \n",
       "16  Here's our plan for Monday after another painf...  20 Hours Ago   \n",
       "17     What to watch in the markets in the week ahead  20 Hours Ago   \n",
       "18  Pro Picks: Watch all of Friday's big stock cal...  20 Hours Ago   \n",
       "19  13 careers where over 50% of workers are happy...  20 Hours Ago   \n",
       "20  New York AG wrongly said Yankees game on Apple...  20 Hours Ago   \n",
       "21  Tech stocks just had the worst two-week stretc...  21 Hours Ago   \n",
       "22  Takeaways from Jim Cramer's interviews with th...  22 Hours Ago   \n",
       "23  Some homebuyers are facing mortgage 'payment s...  22 Hours Ago   \n",
       "24  Ether is down almost 20% since the merge. Here...  22 Hours Ago   \n",
       "25  Elon Musk has over 20 direct reports at Tesla ...  22 Hours Ago   \n",
       "26  Trump SPAC shares are now around $16 after hit...  23 Hours Ago   \n",
       "27  Here's why U.S. fiscal policy is undermining t...  23 Hours Ago   \n",
       "28  Why gold and crypto haven't proven to be 'infl...  23 Hours Ago   \n",
       "29  Amid U.S. recession fears, these 4 steps can h...  23 Hours Ago   \n",
       "\n",
       "                                                  URL  \n",
       "0   https://www.cnbc.com/2022/09/24/biden-administ...  \n",
       "1   https://www.cnbc.com/2022/09/24/top-10-cities-...  \n",
       "2   https://www.cnbc.com/2022/09/24/easy-meals-for...  \n",
       "3   https://www.cnbc.com/2022/09/24/how-to-raise-r...  \n",
       "4   https://www.cnbc.com/2022/09/24/how-airlines-p...  \n",
       "5   https://www.cnbc.com/2022/09/24/queer-eyes-kar...  \n",
       "6   https://www.cnbc.com/2022/09/24/vermont-new-me...  \n",
       "7   https://www.cnbc.com/2022/09/24/new-york-now-n...  \n",
       "8   https://www.cnbc.com/2022/09/24/analysts-name-...  \n",
       "9   https://www.cnbc.com/2022/09/24/feared-stock-m...  \n",
       "10  https://www.cnbc.com/2022/09/24/wallethub-top-...  \n",
       "11  https://www.cnbc.com/2022/09/24/quantum-invest...  \n",
       "12  https://www.cnbc.com/2022/09/24/what-parent-pl...  \n",
       "13  https://www.cnbc.com/2022/09/24/check-in-smoke...  \n",
       "14  https://www.cnbc.com/2022/09/24/convertible-sa...  \n",
       "15  https://www.cnbc.com/2022/09/24/liz-truss-brit...  \n",
       "16  https://www.cnbc.com/2022/09/23/it-was-another...  \n",
       "17  https://www.cnbc.com/2022/09/23/stocks-could-c...  \n",
       "18  https://www.cnbc.com/2022/09/23/pro-picks-watc...  \n",
       "19  https://www.cnbc.com/2022/09/23/careers-where-...  \n",
       "20  https://www.cnbc.com/2022/09/23/new-york-ag-wr...  \n",
       "21  https://www.cnbc.com/2022/09/23/tech-stocks-wo...  \n",
       "22  https://www.cnbc.com/2022/09/23/jim-cramer-sat...  \n",
       "23  https://www.cnbc.com/2022/09/23/some-homebuyer...  \n",
       "24  https://www.cnbc.com/2022/09/23/ether-is-down-...  \n",
       "25  https://www.cnbc.com/2022/09/23/elon-musk-dire...  \n",
       "26  https://www.cnbc.com/2022/09/23/trump-merger-p...  \n",
       "27  https://www.cnbc.com/2022/09/23/us-fiscal-poli...  \n",
       "28  https://www.cnbc.com/2022/09/23/why-gold-and-c...  \n",
       "29  https://www.cnbc.com/2022/09/23/amid-us-recess...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#send get request to webpage server\n",
    "page = requests.get(\"https://www.cnbc.com/world/?region=world\")\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "#initialize list for headline, time and news link as empty\n",
    "headline = []\n",
    "time = []\n",
    "news_link = []\n",
    "\n",
    "#loop in to collect data\n",
    "for i in soup.find_all('li',class_='LatestNews-item'):\n",
    "    headline.append(i.find('a',class_='LatestNews-headline').get_text().strip())\n",
    "    news_link.append(i.find('a', class_='LatestNews-headline', href=True).get('href'))\n",
    "    time.append(i.find('time',class_='LatestNews-timestamp').get_text().strip())\n",
    "\n",
    "#create a dataframe\n",
    "df = pd.DataFrame({'Headline':headline, 'Time':time, 'URL':news_link})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64d1a9f",
   "metadata": {},
   "source": [
    "# 8) To scrape the details of most downloaded articles from AI in last 90 days. https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles Paper Title, Authors, Published Date, Paper URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48d4e3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Publish Date</th>\n",
       "      <th>Paper URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reward is enough</td>\n",
       "      <td>Silver, David, Singh, Satinder, Precup, Doina,...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Making sense of raw input</td>\n",
       "      <td>Evans, Richard, Bošnjak, Matko and 5 more</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Law and logic: A review from an argumentation ...</td>\n",
       "      <td>Prakken, Henry, Sartor, Giovanni</td>\n",
       "      <td>October 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Creativity and artificial intelligence</td>\n",
       "      <td>Boden, Margaret A.</td>\n",
       "      <td>August 1998</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Artificial cognition for social human–robot in...</td>\n",
       "      <td>Lemaignan, Séverin, Warnier, Mathieu and 3 more</td>\n",
       "      <td>June 2017</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Explanation in artificial intelligence: Insigh...</td>\n",
       "      <td>Miller, Tim</td>\n",
       "      <td>February 2019</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Making sense of sensory input</td>\n",
       "      <td>Evans, Richard, Hernández-Orallo, José and 3 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Conflict-based search for optimal multi-agent ...</td>\n",
       "      <td>Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...</td>\n",
       "      <td>February 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Between MDPs and semi-MDPs: A framework for te...</td>\n",
       "      <td>Sutton, Richard S., Precup, Doina, Singh, Sati...</td>\n",
       "      <td>August 1999</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Hanabi challenge: A new frontier for AI re...</td>\n",
       "      <td>Bard, Nolan, Foerster, Jakob N. and 13 more</td>\n",
       "      <td>March 2020</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Evaluating XAI: A comparison of rule-based and...</td>\n",
       "      <td>van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...</td>\n",
       "      <td>February 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Argumentation in artificial intelligence</td>\n",
       "      <td>Bench-Capon, T.J.M., Dunne, Paul E.</td>\n",
       "      <td>October 2007</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Algorithms for computing strategies in two-pla...</td>\n",
       "      <td>Bošanský, Branislav, Lisý, Viliam and 3 more</td>\n",
       "      <td>August 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Multiple object tracking: A literature review</td>\n",
       "      <td>Luo, Wenhan, Xing, Junliang and 4 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Selection of relevant features and examples in...</td>\n",
       "      <td>Blum, Avrim L., Langley, Pat</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>A survey of inverse reinforcement learning: Ch...</td>\n",
       "      <td>Arora, Saurabh, Doshi, Prashant</td>\n",
       "      <td>August 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Explaining individual predictions when feature...</td>\n",
       "      <td>Aas, Kjersti, Jullum, Martin, Løland, Anders</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A review of possible effects of cognitive bias...</td>\n",
       "      <td>Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...</td>\n",
       "      <td>June 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Integrating social power into the decision-mak...</td>\n",
       "      <td>Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.</td>\n",
       "      <td>December 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>“That's (not) the output I expected!” On the r...</td>\n",
       "      <td>Riveiro, Maria, Thill, Serge</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Explaining black-box classifiers using post-ho...</td>\n",
       "      <td>Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...</td>\n",
       "      <td>May 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Algorithm runtime prediction: Methods &amp; evalua...</td>\n",
       "      <td>Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...</td>\n",
       "      <td>January 2014</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Wrappers for feature subset selection</td>\n",
       "      <td>Kohavi, Ron, John, George H.</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Commonsense visual sensemaking for autonomous ...</td>\n",
       "      <td>Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Quantum computation, quantum theory and AI</td>\n",
       "      <td>Ying, Mingsheng</td>\n",
       "      <td>February 2010</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Paper Title  \\\n",
       "0                                    Reward is enough   \n",
       "1                           Making sense of raw input   \n",
       "2   Law and logic: A review from an argumentation ...   \n",
       "3              Creativity and artificial intelligence   \n",
       "4   Artificial cognition for social human–robot in...   \n",
       "5   Explanation in artificial intelligence: Insigh...   \n",
       "6                       Making sense of sensory input   \n",
       "7   Conflict-based search for optimal multi-agent ...   \n",
       "8   Between MDPs and semi-MDPs: A framework for te...   \n",
       "9   The Hanabi challenge: A new frontier for AI re...   \n",
       "10  Evaluating XAI: A comparison of rule-based and...   \n",
       "11           Argumentation in artificial intelligence   \n",
       "12  Algorithms for computing strategies in two-pla...   \n",
       "13      Multiple object tracking: A literature review   \n",
       "14  Selection of relevant features and examples in...   \n",
       "15  A survey of inverse reinforcement learning: Ch...   \n",
       "16  Explaining individual predictions when feature...   \n",
       "17  A review of possible effects of cognitive bias...   \n",
       "18  Integrating social power into the decision-mak...   \n",
       "19  “That's (not) the output I expected!” On the r...   \n",
       "20  Explaining black-box classifiers using post-ho...   \n",
       "21  Algorithm runtime prediction: Methods & evalua...   \n",
       "22              Wrappers for feature subset selection   \n",
       "23  Commonsense visual sensemaking for autonomous ...   \n",
       "24         Quantum computation, quantum theory and AI   \n",
       "\n",
       "                                              Authors    Publish Date  \\\n",
       "0   Silver, David, Singh, Satinder, Precup, Doina,...    October 2021   \n",
       "1           Evans, Richard, Bošnjak, Matko and 5 more    October 2021   \n",
       "2                    Prakken, Henry, Sartor, Giovanni    October 2015   \n",
       "3                                  Boden, Margaret A.     August 1998   \n",
       "4     Lemaignan, Séverin, Warnier, Mathieu and 3 more       June 2017   \n",
       "5                                         Miller, Tim   February 2019   \n",
       "6   Evans, Richard, Hernández-Orallo, José and 3 more      April 2021   \n",
       "7   Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...   February 2015   \n",
       "8   Sutton, Richard S., Precup, Doina, Singh, Sati...     August 1999   \n",
       "9         Bard, Nolan, Foerster, Jakob N. and 13 more      March 2020   \n",
       "10  van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...   February 2021   \n",
       "11                Bench-Capon, T.J.M., Dunne, Paul E.    October 2007   \n",
       "12       Bošanský, Branislav, Lisý, Viliam and 3 more     August 2016   \n",
       "13             Luo, Wenhan, Xing, Junliang and 4 more      April 2021   \n",
       "14                       Blum, Avrim L., Langley, Pat   December 1997   \n",
       "15                    Arora, Saurabh, Doshi, Prashant     August 2021   \n",
       "16       Aas, Kjersti, Jullum, Martin, Løland, Anders  September 2021   \n",
       "17  Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...       June 2021   \n",
       "18     Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.   December 2016   \n",
       "19                       Riveiro, Maria, Thill, Serge  September 2021   \n",
       "20  Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...        May 2021   \n",
       "21  Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...    January 2014   \n",
       "22                       Kohavi, Ron, John, George H.   December 1997   \n",
       "23  Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...    October 2021   \n",
       "24                                    Ying, Mingsheng   February 2010   \n",
       "\n",
       "                                            Paper URL  \n",
       "0   https://www.sciencedirect.com/science/article/...  \n",
       "1   https://www.sciencedirect.com/science/article/...  \n",
       "2   https://www.sciencedirect.com/science/article/...  \n",
       "3   https://www.sciencedirect.com/science/article/...  \n",
       "4   https://www.sciencedirect.com/science/article/...  \n",
       "5   https://www.sciencedirect.com/science/article/...  \n",
       "6   https://www.sciencedirect.com/science/article/...  \n",
       "7   https://www.sciencedirect.com/science/article/...  \n",
       "8   https://www.sciencedirect.com/science/article/...  \n",
       "9   https://www.sciencedirect.com/science/article/...  \n",
       "10  https://www.sciencedirect.com/science/article/...  \n",
       "11  https://www.sciencedirect.com/science/article/...  \n",
       "12  https://www.sciencedirect.com/science/article/...  \n",
       "13  https://www.sciencedirect.com/science/article/...  \n",
       "14  https://www.sciencedirect.com/science/article/...  \n",
       "15  https://www.sciencedirect.com/science/article/...  \n",
       "16  https://www.sciencedirect.com/science/article/...  \n",
       "17  https://www.sciencedirect.com/science/article/...  \n",
       "18  https://www.sciencedirect.com/science/article/...  \n",
       "19  https://www.sciencedirect.com/science/article/...  \n",
       "20  https://www.sciencedirect.com/science/article/...  \n",
       "21  https://www.sciencedirect.com/science/article/...  \n",
       "22  https://www.sciencedirect.com/science/article/...  \n",
       "23  https://www.sciencedirect.com/science/article/...  \n",
       "24  https://www.sciencedirect.com/science/article/...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#send get request to webpage server\n",
    "page = requests.get(\"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\")\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "#initialize list as empty\n",
    "title = []\n",
    "authors = []\n",
    "publish_date = []\n",
    "paper_url = []\n",
    "\n",
    "#loop in to collect data\n",
    "for i in soup.find_all('li',class_='sc-9zxyh7-1 sc-9zxyh7-2 exAXfr jQmQZp'):\n",
    "    title.append(i.find('h2',class_='sc-1qrq3sd-1 MKjKb sc-1nmom32-0 sc-1nmom32-1 hqhUYH ebTA-dR').get_text().strip())\n",
    "    authors.append(i.find('span',class_='sc-1w3fpd7-0 pgLAT').get_text().strip())\n",
    "    paper_url.append(i.find('a', class_='sc-5smygv-0 nrDZj', href=True).get('href'))\n",
    "    publish_date.append(i.find('span',class_='sc-1thf9ly-2 bKddwo').get_text().strip())\n",
    "\n",
    "#create a dataframe\n",
    "df = pd.DataFrame({'Paper Title':title, 'Authors':authors, 'Publish Date':publish_date, 'Paper URL':paper_url})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0446e9",
   "metadata": {},
   "source": [
    "# 9) To scrape mentioned details from dineout.co.in :Restaurant name, Cuisine, Location, Ratings, Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "553da1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant Name</th>\n",
       "      <th>Cusines</th>\n",
       "      <th>Location</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Img URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Castle Barbeque</td>\n",
       "      <td>Chinese, North Indian</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jungle Jamboree</td>\n",
       "      <td>North Indian, Asian, Italian</td>\n",
       "      <td>3CS Mall,Lajpat Nagar - 3, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Castle Barbeque</td>\n",
       "      <td>Chinese, North Indian</td>\n",
       "      <td>Pacific Mall,Tagore Garden, West Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cafe Knosh</td>\n",
       "      <td>Italian, Continental</td>\n",
       "      <td>The Leela Ambience Convention Hotel,Shahdara, ...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Barbeque Company</td>\n",
       "      <td>North Indian, Chinese</td>\n",
       "      <td>Gardens Galleria,Sector 38A, Noida</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>India Grill</td>\n",
       "      <td>North Indian, Italian</td>\n",
       "      <td>Hilton Garden Inn,Saket, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Delhi Barbeque</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>Taurus Sarovar Portico,Mahipalpur, South Delhi</td>\n",
       "      <td>3.7</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Monarch - Bar Be Que Village</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>Indirapuram Habitat Centre,Indirapuram, Ghaziabad</td>\n",
       "      <td>3.8</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Indian Grill Room</td>\n",
       "      <td>North Indian, Mughlai</td>\n",
       "      <td>Suncity Business Tower,Golf Course Road, Gurgaon</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Restaurant Name                        Cusines  \\\n",
       "0                   Castle Barbeque          Chinese, North Indian   \n",
       "1                   Jungle Jamboree   North Indian, Asian, Italian   \n",
       "2                   Castle Barbeque          Chinese, North Indian   \n",
       "3                        Cafe Knosh           Italian, Continental   \n",
       "4              The Barbeque Company          North Indian, Chinese   \n",
       "5                       India Grill          North Indian, Italian   \n",
       "6                    Delhi Barbeque                   North Indian   \n",
       "7  The Monarch - Bar Be Que Village                   North Indian   \n",
       "8                 Indian Grill Room          North Indian, Mughlai   \n",
       "\n",
       "                                            Location Rating  \\\n",
       "0                     Connaught Place, Central Delhi    4.1   \n",
       "1             3CS Mall,Lajpat Nagar - 3, South Delhi    3.9   \n",
       "2             Pacific Mall,Tagore Garden, West Delhi    3.9   \n",
       "3  The Leela Ambience Convention Hotel,Shahdara, ...    4.3   \n",
       "4                 Gardens Galleria,Sector 38A, Noida      4   \n",
       "5               Hilton Garden Inn,Saket, South Delhi    3.9   \n",
       "6     Taurus Sarovar Portico,Mahipalpur, South Delhi    3.7   \n",
       "7  Indirapuram Habitat Centre,Indirapuram, Ghaziabad    3.8   \n",
       "8   Suncity Business Tower,Golf Course Road, Gurgaon    4.3   \n",
       "\n",
       "                                             Img URL  \n",
       "0  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "1  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "2  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "3  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "4  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "5  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "6  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "7  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "8  https://im1.dineout.co.in/images/uploads/resta...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#send get request to webpage server\n",
    "page = requests.get(\"https://www.dineout.co.in/delhi-restaurants/buffet-special\")\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "#initialize list as empty\n",
    "restaurant_name = []\n",
    "cuisine = []\n",
    "location = []\n",
    "rating = []\n",
    "image_url = []\n",
    "\n",
    "#loop in to collect data\n",
    "for i in soup.find_all('div',class_='restnt-main-wrap clearfix'):\n",
    "    restaurant_name.append(i.find('a',class_='restnt-name ellipsis').get_text().strip())\n",
    "    cuisine.append(i.find('span',class_='double-line-ellipsis').get_text().strip().split('|')[1])\n",
    "    rating.append(i.find('div',class_='restnt-rating rating-4').get_text().strip())\n",
    "    location.append(i.find('div',class_='restnt-loc ellipsis').get_text().strip())\n",
    "    image_url.append(i.find('img', class_='no-img')['data-src'])\n",
    "                     \n",
    "#create a dataframe\n",
    "df = pd.DataFrame({'Restaurant Name':restaurant_name, 'Cusines':cuisine, 'Location':location, 'Rating':rating, 'Img URL':image_url})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aab5216",
   "metadata": {},
   "source": [
    "# 10) To scrape the details of top publications from Google Scholar from https://scholar.google.com/citations?view_op=top_venues&hl=en Rank, Publication, h5-index, h5-median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3b55a4a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Publication</th>\n",
       "      <th>h5-index</th>\n",
       "      <th>h5-median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Nature</td>\n",
       "      <td>444</td>\n",
       "      <td>667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The New England Journal of Medicine</td>\n",
       "      <td>432</td>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Science</td>\n",
       "      <td>401</td>\n",
       "      <td>614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>IEEE/CVF Conference on Computer Vision and Pat...</td>\n",
       "      <td>389</td>\n",
       "      <td>627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>The Lancet</td>\n",
       "      <td>354</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>Journal of Business Research</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>Molecular Cancer</td>\n",
       "      <td>145</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>Sensors</td>\n",
       "      <td>145</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>Nature Climate Change</td>\n",
       "      <td>144</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>IEEE Internet of Things Journal</td>\n",
       "      <td>144</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                                        Publication h5-index h5-median\n",
       "0     1                                             Nature      444       667\n",
       "1     2                The New England Journal of Medicine      432       780\n",
       "2     3                                            Science      401       614\n",
       "3     4  IEEE/CVF Conference on Computer Vision and Pat...      389       627\n",
       "4     5                                         The Lancet      354       635\n",
       "..  ...                                                ...      ...       ...\n",
       "95   96                       Journal of Business Research      145       233\n",
       "96   97                                   Molecular Cancer      145       209\n",
       "97   98                                            Sensors      145       201\n",
       "98   99                              Nature Climate Change      144       228\n",
       "99  100                    IEEE Internet of Things Journal      144       212\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#send get request to webpage server\n",
    "page = requests.get(\"https://scholar.google.com/citations?view_op=top_venues&hl=en\")\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "#initialize list as empty\n",
    "rank = []\n",
    "publication = []\n",
    "h5_index = []\n",
    "h5_median = []\n",
    "\n",
    "table = soup.find('table',class_=\"gsc_mp_table\")\n",
    "\n",
    "count = 1\n",
    "\n",
    "#loop in to collect data\n",
    "for rows in table.find_all('tr'):\n",
    "    if (count == 1):\n",
    "        count = count + 1\n",
    "        continue\n",
    "    col = rows.find_all('td')\n",
    "    rank.append(col[0].get_text().strip().replace('.',''))\n",
    "    publication.append(col[1].get_text().strip())\n",
    "    h5_index.append(col[2].get_text().strip())\n",
    "    h5_median.append(col[3].get_text().strip())\n",
    "\n",
    "                     \n",
    "#create a dataframe\n",
    "df = pd.DataFrame({'Rank':rank, 'Publication':publication, 'h5-index':h5_index, 'h5-median':h5_median})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e50ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
